imports:
  root: ../__package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    calculate:
      path: /v1/convai/llm-usage/calculate
      method: POST
      auth: false
      docs: >-
        Returns a list of LLM models and the expected cost for using them based
        on the provided values.
      source:
        openapi: openapi.json
      display-name: Calculate Expected Llm Usage
      request:
        name: LlmUsageCalculatorPublicRequestModel
        body:
          properties:
            prompt_length:
              type: integer
              docs: Length of the prompt in characters.
            number_of_pages:
              type: integer
              docs: >-
                Pages of content in PDF documents or URLs in the agent's
                knowledge base.
            rag_enabled:
              type: boolean
              docs: Whether RAG is enabled.
        content-type: application/json
      response:
        docs: Successful Response
        type: root.LlmUsageCalculatorResponseModel
        status-code: 200
      errors:
        - root.UnprocessableEntityError
      examples:
        - request:
            prompt_length: 1
            number_of_pages: 1
            rag_enabled: true
          response:
            body:
              llm_prices:
                - llm: gpt-4o-mini
                  price_per_minute: 1.1
      audiences:
        - convai
  source:
    openapi: openapi.json
