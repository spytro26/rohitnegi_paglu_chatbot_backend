imports:
  root: ../../__package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    calculate:
      path: /v1/convai/agent/{agent_id}/llm-usage/calculate
      method: POST
      auth: false
      docs: Calculates expected number of LLM tokens needed for the specified agent.
      source:
        openapi: openapi.json
      path-parameters:
        agent_id: string
      display-name: Calculate Expected Llm Usage For An Agent
      request:
        name: LlmUsageCalculatorRequestModel
        body:
          properties:
            prompt_length:
              type: optional<integer>
              docs: Length of the prompt in characters.
            number_of_pages:
              type: optional<integer>
              docs: >-
                Pages of content in pdf documents OR urls in agent's Knowledge
                Base.
            rag_enabled:
              type: optional<boolean>
              docs: Whether RAG is enabled.
        content-type: application/json
      response:
        docs: Successful Response
        type: root.LlmUsageCalculatorResponseModel
        status-code: 200
      errors:
        - root.UnprocessableEntityError
      examples:
        - path-parameters:
            agent_id: agent_id
          request: {}
          response:
            body:
              llm_prices:
                - llm: gpt-4o-mini
                  price_per_minute: 1.1
      audiences:
        - convai
  source:
    openapi: openapi.json
